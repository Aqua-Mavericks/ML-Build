# ğŸš° Phase 2 â€” Anomaly Detection (Industrial Water Quality Monitoring)

> **Short take:** this phase builds the system that notices when water or sensors act weird â€” not later, not after someone sees a chart, but **now** âš¡.  
It uses **feature engineering + anomaly models + correlation-aware rules** to surface *real problems* while avoiding **alarm fatigue** ğŸš¨.

---

## ğŸ›£ï¸ Roadmap 

1. ğŸ”— **Correlation-first checks** â€” enforce physical/chemical rules between pH, TDS, Turbidity, and Temperature.  
2. ğŸ¤– **Unsupervised baseline** â€” Isolation Forest + lightweight statistical guards.  
3. ğŸ‘©â€ğŸ’» **Operator-in-the-loop** â€” ops feedback â†’ refine thresholds â†’ cut false positives.  
4. ğŸ“¦ **Edge-ready rollouts** â€” ONNX/Joblib artifacts with logging + safe fallbacks.  

---

## ğŸ¯ Goal

Detect **abnormal behaviour** in the four core water signals â€”  
ğŸŒ¡ï¸ Temperature, ğŸ§‚ TDS, ğŸŒ«ï¸ Turbidity, âš—ï¸ pH â€”  
and provide **clear, explainable alerts** that operators can act on instantly.  

ğŸ’¡ **Real-world uses:** leak detection, contamination alerts, sensor-fault detection, predictive maintenance, compliance evidence.

---

## ğŸ““ Current Implementation (Notebook: `water-anomaly.ipynb`)  

âœ… **Libraries:** `pandas`, `scikit-learn (IsolationForest)`, `joblib`  
âœ… **Data pipeline:** CSV â†’ timestamp parsing â†’ chronological sort  
âœ… **Engineered features:**  
- `ph_dev` â†’ deviation from neutral pH (7)  
- `tds_temp_ratio` â†’ ratio of TDS to temperature  
- `turbidity_x_ph` â†’ turbidity Ã— pH  
- `temp_sqr` â†’ squared temperature (nonlinear effect)  

ğŸ¤– **Model:**  
- Isolation Forest (`n_estimators=100`, `contamination=0.05`, `random_state=42`)  
- Trained on engineered features  

ğŸ“Š **Outputs:**  
- `anomaly_score` â†’ continuous decision function  
- `anomaly_raw` â†’ raw prediction (`-1` anomaly, `1` normal)  
- `anomaly_status` â†’ mapped (`Anomaly` / `Normal`)  

---

## ğŸ”— Correlation & Interdependency  

Sensors interact like a **mini chemical system** ğŸ§ª. We test if readings fit expected patterns.  

| Trigger              |        pH | Temperature |   TDS | Turbidity |
| -------------------- | --------: | ----------: | ----: | --------: |
| pH â†‘ (alkaline)      |        â€”  | slight â†‘   | salts â†’ â†‘/â†“ | â†“ microbes â†’ â†“ |
| pH â†“ (acidic)        |        â€”  | slight â†“   | solids dissolve â†’ â†‘ | organics decay â†’ â†‘ |
| Temp â†‘               | small â†“   |        â€”   | dissolve solids â†’ â†‘ | microbial growth â†’ â†‘ |
| Temp â†“               | small â†‘   |        â€”   | crystallization â†’ â†“ | less bioactivity â†’ â†“ |
| TDS â†‘                | buffered  | often â†‘    |    â€”   | suspended load â†’ â†‘ |
| Turbidity â†‘          | decay â†’ â†“ | slight â†‘   | affects TDS readings | â€” |

âš ï¸ **Example rule:** If pH â‰ˆ 9 & Temp â‰ˆ 50Â°C â†’ ğŸš© mismatch (chemically odd).

---

## ğŸ§© Engineered Features (Why they matter)

- ğŸ“‰ **Rolling stats** (`*_rollmean_k`, `*_rollstd_k`) â†’ smooth baselines.  
- ğŸ”€ **Deltas & rates** (`*_diff1`, `*_rate`) â†’ sudden jumps/drifts.  
- ğŸ”— **Interactions** (e.g. `tds_temp_ratio`) â†’ capture chemistry logic.  
- ğŸ“ **Robust z-scores/IQR** â†’ outlier guards.  

> ğŸ—‚ï¸ Keep a **versioned config** so inference = training features.

---

## âš™ï¸ Step-by-step Pipeline  

1. â³ Ingest & parse timestamps â†’ keep order.  
2. ğŸ§¹ Clean missing/dup values â†’ safe forward/backfill.  
3. â±ï¸ Resample â†’ fixed cadence (1â€“5 min).  
4. ğŸ› ï¸ Engineer features â†’ rolling stats, deltas, ratios.  
5. ğŸ“Š Train-test split (chronological).  
6. ğŸ“ Fit scaler â†’ persist.  
7. ğŸ¤– Train Isolation Forest + stats thresholds.  
8. âš–ï¸ Calibrate thresholds â†’ add correlation rules.  
9. ğŸ§ª Validate with synthetic anomalies (spikes, drifts, flatlines).  
10. ğŸš€ Export model + config + correlation matrix.  

---

## ğŸ“ˆ Evaluation & Metrics  

- If **labeled**: Precision, Recall, F1, PR-AUC, Latency.  
- If **unlabeled**: operator review + backtests with injected anomalies.  
- Always visualize: ğŸ“‰ anomaly scores vs. timeline + ğŸš© flagged events.

---

## ğŸ› ï¸ Ops & Monitoring  

- ğŸ“Š Drift monitors â†’ mean/variance per signal.  
- â° Daily checks â†’ anomaly count, latency.  
- ğŸ“ Logging â†’ input + score + correlation flags.  
- â™»ï¸ Retraining triggers â†’ drift or false positive creep.  

---

## ğŸš§ Known Limitations  

- âš¡ **Unsupervised noise:** rare but valid states flagged â†’ mitigate with debounce + operator feedback.  
- ğŸŒ **Site-specific chemistry:** correlation baselines need tuning.  
- ğŸ›‘ **Sensor faults:** solve with cross-sensor consistency, not single-sensor alarms.  

---

## ğŸ‘¥ Authors  

- **Pushkar Arora**  
  - [GitHub](https://github.com/pushkar1887) | [LinkedIn](https://www.linkedin.com/in/pushkar-arora-0b3599356/) | [Kaggle](https://www.kaggle.com/pushkararora)  

- **Satyam Choudhary**  
  - [GitHub](https://github.com/SatyamChoudhary1909) | [LinkedIn](https://www.linkedin.com/in/satyam-choudhary-114b89325/)  

---
